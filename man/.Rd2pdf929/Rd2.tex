\documentclass[a4paper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\usepackage[latin1]{inputenc} % @SET ENCODING@
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge \R{} documentation}} \par\bigskip{{\Large of \file{erboost.Rd} etc.}}
\par\bigskip{\large \today}
\end{center}
\inputencoding{utf8}
\HeaderA{erboost}{ER-Boost Expectile Regression Modeling}{erboost}
\methaliasA{erboost.fit}{erboost}{erboost.fit}
\methaliasA{erboost.more}{erboost}{erboost.more}
\keyword{models}{erboost}
\keyword{nonlinear}{erboost}
\keyword{survival}{erboost}
\keyword{nonparametric}{erboost}
\keyword{tree}{erboost}
%
\begin{Description}\relax
Fits ER-Boost Expectile Regression models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
erboost(formula = formula(data),
    distribution = list(name="expectile",alpha=0.5),
    data = list(),
    weights,
    var.monotone = NULL,
    n.trees = 100,
    interaction.depth = 1,
    n.minobsinnode = 10,
    shrinkage = 0.001,
    bag.fraction = 0.5,
    train.fraction = 1.0,
    cv.folds=0,
    keep.data = TRUE,
    verbose = TRUE)

erboost.fit(x,y,
        offset = NULL,
        misc = NULL,
        distribution = list(name="expectile",alpha=0.5),
        w = NULL,
        var.monotone = NULL,
        n.trees = 100,
        interaction.depth = 1,
        n.minobsinnode = 10,
        shrinkage = 0.001,
        bag.fraction = 0.5,
        train.fraction = 1.0,
        keep.data = TRUE,
        verbose = TRUE,
        var.names = NULL,
        response.name = NULL)

erboost.more(object,
         n.new.trees = 100,
         data = NULL,
         weights = NULL,
         offset = NULL,
         verbose = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a symbolic description of the model to be fit. The formula may 
include an offset term (e.g. y\textasciitilde{}offset(n)+x). If \code{keep.data=FALSE} in 
the initial call to \code{erboost} then it is the user's responsibility to 
resupply the offset to \code{\LinkA{erboost.more}{erboost.more}}.
\item[\code{distribution}] a list with a component \code{name} specifying the distribution 
and any additional parameters needed.Expectile regression is available and \code{distribution} must a list of the form 
\code{list(name="expectile",alpha=0.25)} where \code{alpha} is the expectile 
to estimate. The current version's expectile regression methods do 
not handle non-constant weights and will stop.
\item[\code{data}] an optional data frame containing the variables in the model. By
default the variables are taken from \code{environment(formula)}, typically 
the environment from which \code{erboost} is called. If \code{keep.data=TRUE} in 
the initial call to \code{erboost} then \code{erboost} stores a copy with the 
object. If \code{keep.data=FALSE} then subsequent calls to 
\code{\LinkA{erboost.more}{erboost.more}} must resupply the same dataset. It becomes the user's 
responsibility to resupply the same data at this point.
\item[\code{weights}] an optional vector of weights to be used in the fitting process. 
Must be positive but do not need to be normalized. If \code{keep.data=FALSE} 
in the initial call to \code{erboost} then it is the user's responsibility to 
resupply the weights to \code{\LinkA{erboost.more}{erboost.more}}.
\item[\code{var.monotone}] an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.
\item[\code{n.trees}] the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion.
\item[\code{cv.folds}] Number of cross-validation folds to perform. If \code{cv.folds}>1 then
\code{erboost}, in addition to the usual fit, will perform a cross-validation, calculate
an estimate of generalization error returned in \code{cv.error}.
\item[\code{interaction.depth}] The maximum depth of variable interactions. 1 implies
an additive model, 2 implies a model with up to 2-way interactions, etc.
\item[\code{n.minobsinnode}] minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight.
\item[\code{shrinkage}] a shrinkage parameter applied to each tree in the expansion.
Also known as the learning rate or step-size reduction.
\item[\code{bag.fraction}] the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces randomnesses
into the model fit. If \code{bag.fraction}<1 then running the same model twice
will result in similar but different fits. \code{erboost} uses the R random number
generator so \code{set.seed} can ensure that the model can be
reconstructed. Preferably, the user can save the returned
\code{\LinkA{erboost.object}{erboost.object}} using \code{\LinkA{save}{save}}.
\item[\code{train.fraction}] The first \code{train.fraction * nrows(data)}
observations are used to fit the \code{erboost} and the remainder are used for
computing out-of-sample estimates of the loss function.
\item[\code{keep.data}] a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index makes
subsequent calls to \code{\LinkA{erboost.more}{erboost.more}} faster at the cost of storing an
extra copy of the dataset.
\item[\code{object}] a \code{erboost} object created from an initial call to
\code{\LinkA{erboost}{erboost}}.
\item[\code{n.new.trees}] the number of additional trees to add to \code{object}.
\item[\code{verbose}] If TRUE, erboost will print out progress and performance indicators.
If this option is left unspecified for erboost.more then it uses \code{verbose} from
\code{object}.

\item[\code{x, y}] For \code{erboost.fit}: \code{x} is a data frame or data matrix containing the
predictor variables and \code{y} is the vector of outcomes. The number of rows
in \code{x} must be the same as the length of \code{y}.
\item[\code{offset}] a vector of values for the offset
\item[\code{misc}] For \code{erboost.fit}: \code{misc} is an R object that is simply passed on to
the erboost engine.
\item[\code{w}] For \code{erboost.fit}: \code{w} is a vector of weights of the same
length as the \code{y}.
\item[\code{var.names}] For \code{erboost.fit}: A vector of strings of length equal to the
number of columns of \code{x} containing the names of the predictor variables.
\item[\code{response.name}] For \code{erboost.fit}: A character string label for the response
variable.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This package implements a regression tree based gradient boosting estimator for nonparametric multivariate expectile regression. The code is a modified version of gbm library originally written by Greg Ridgeway.
Boosting is the process of iteratively adding basis functions in a greedy
fashion so that each additional basis function further reduces the selected
loss function. This implementation closely follows Friedman's Gradient
Boosting Machine (Friedman, 2001).

In addition to many of the features documented in the Gradient Boosting Machine,
\code{erboost} offers additional features including the out-of-bag estimator for
the optimal number of iterations, the ability to store and manipulate the
resulting \code{erboost} object.

\code{erboost.fit} provides the link between R and the C++ erboost engine. \code{erboost}
is a front-end to \code{erboost.fit} that uses the familiar R modeling formulas.
However, \code{\LinkA{model.frame}{model.frame}} is very slow if there are many
predictor variables. For power-users with many variables use \code{erboost.fit}.
For general practice \code{erboost} is preferable.

\end{Details}
%
\begin{Value}
\code{erboost}, \code{erboost.fit}, and \code{erboost.more} return a
\code{\LinkA{erboost.object}{erboost.object}}.
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}

Quantile regression code developed by Brian Kriegler \email{bk@stat.ucla.edu}
\end{Author}
%
\begin{References}\relax
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\\{}
Y. Freund and R.E. Schapire (1997) ``A decision-theoretic generalization of
on-line learning and an application to boosting,'' \emph{Journal of Computer and
System Sciences,} 55(1):119-139.

G. Ridgeway (1999). ``The state of boosting,'' \emph{Computing Science and
Statistics} 31:172-181.

J.H. Friedman, T. Hastie, R. Tibshirani (2000). ``Additive Logistic Regression:
a Statistical View of Boosting,'' \emph{Annals of Statistics} 28(2):337-374.

J.H. Friedman (2001). ``Greedy Function Approximation: A Gradient Boosting
Machine,'' \emph{Annals of Statistics} 29(5):1189-1232.

J.H. Friedman (2002). ``Stochastic Gradient Boosting,'' \emph{Computational Statistics
and Data Analysis} 38(4):367-378.

B. Kriegler (2007). \emph{Cost-Sensitive Stochastic Gradient Boosting Within a 
Quantitative Regression Framework}. PhD dissertation, UCLA Statistics. 
\url{http://theses.stat.ucla.edu/57/KrieglerDissertation.pdf}

\url{http://www.i-pensieri.com/gregr/erboost.shtml}

\url{http://www-stat.stanford.edu/~jhf/R-MART.html}
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{erboost.object}{erboost.object}},
\code{\LinkA{erboost.perf}{erboost.perf}},
\code{\LinkA{plot.erboost}{plot.erboost}},
\code{\LinkA{predict.erboost}{predict.erboost}},
\code{\LinkA{summary.erboost}{summary.erboost}},
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

N <- 1000
X1 <- runif(N)
X2 <- 2*runif(N)
X3 <- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])
X4 <- factor(sample(letters[1:6],N,replace=TRUE))
X5 <- factor(sample(letters[1:3],N,replace=TRUE))
X6 <- 3*runif(N)
mu <- c(-1,0,1,2)[as.numeric(X3)]

SNR <- 10 # signal-to-noise ratio
Y <- X1**1.5 + 2 * (X2**.5) + mu
sigma <- sqrt(var(Y)/SNR)
Y <- Y + rnorm(N,0,sigma)

# introduce some missing values
X1[sample(1:N,size=500)] <- NA
X4[sample(1:N,size=300)] <- NA

data <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

# fit initial model
erboost1 <- erboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data,                   # dataset
    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
                                 # +1: monotone increase,
                                 #  0: no monotone restrictions
    distribution=list(name="expectile",alpha=0.5),
                                 # expectile, quantile
    n.trees=3000,                # number of trees
    shrinkage=0.005,             # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 5,                # do 5-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=TRUE)                # print out progress


# check performance using a 50% heldout test set
best.iter <- erboost.perf(erboost1,method="test")
print(best.iter)

# check performance using 5-fold cross-validation
best.iter <- erboost.perf(erboost1,method="cv")
print(best.iter)

# plot the performance
# plot variable influence
summary(erboost1,n.trees=1)         # based on the first tree
summary(erboost1,n.trees=best.iter) # based on the estimated best number of trees

# make some new data
N <- 1000
X1 <- runif(N)
X2 <- 2*runif(N)
X3 <- ordered(sample(letters[1:4],N,replace=TRUE))
X4 <- factor(sample(letters[1:6],N,replace=TRUE))
X5 <- factor(sample(letters[1:3],N,replace=TRUE))
X6 <- 3*runif(N)
mu <- c(-1,0,1,2)[as.numeric(X3)]

Y <- X1**1.5 + 2 * (X2**.5) + mu + rnorm(N,0,sigma)

data2 <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale (logit,log,etc.)
f.predict <- predict.erboost(erboost1,data2,best.iter)

# least squares error
print(sum((data2$Y-f.predict)^2))

# create marginal plots
# plot variable X1,X2,X3 after "best" iterations
par(mfrow=c(1,3))
plot.erboost(erboost1,1,best.iter)
plot.erboost(erboost1,2,best.iter)
plot.erboost(erboost1,3,best.iter)
par(mfrow=c(1,1))
# contour plot of variables 1 and 2 after "best" iterations
plot.erboost(erboost1,1:2,best.iter)
# lattice plot of variables 2 and 3
plot.erboost(erboost1,2:3,best.iter)
# lattice plot of variables 3 and 4
plot.erboost(erboost1,3:4,best.iter)

# 3-way plots
plot.erboost(erboost1,c(1,2,6),best.iter,cont=20)
plot.erboost(erboost1,1:3,best.iter)
plot.erboost(erboost1,2:4,best.iter)
plot.erboost(erboost1,3:5,best.iter)

# do another 100 iterations
erboost2 <- erboost.more(erboost1,100,
                 verbose=FALSE) # stop printing detailed progress
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{erboost.object}{ER-Boost Expectile Regression Model Object}{erboost.object}
\keyword{methods}{erboost.object}
%
\begin{Description}\relax
These are objects representing fitted \code{erboost}s.
\end{Description}
%
\begin{Value}
\begin{ldescription}
\item[\code{initF}] the "intercept" term, the initial predicted value to which trees
make adjustments
\item[\code{fit}] a vector containing the fitted values on the scale of regression
function
\item[\code{train.error}] a vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the training data
\item[\code{valid.error}] a vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the validation data
\item[\code{cv.error}] if \code{cv.folds}<2 this component is NULL. Otherwise, this 
component is a vector of length equal to the number of fitted trees
containing a cross-validated estimate of the loss function for each boosting 
iteration
\item[\code{oobag.improve}] a vector of length equal to the number of fitted trees
containing an out-of-bag estimate of the marginal reduction in the expected
value of the loss function. The out-of-bag estimate uses only the training
data and is useful for estimating the optimal number of boosting iterations.
See \code{\LinkA{erboost.perf}{erboost.perf}}
\item[\code{trees}] a list containing the tree structures.
\item[\code{c.splits}] a list of all the categorical splits in the collection of
trees. If the \code{trees[[i]]} component of a \code{erboost} object describes a
categorical split then the splitting value will refer to a component of
\code{c.splits}. That component of \code{c.splits} will be a vector of length
equal to the number of levels in the categorical split variable. -1 indicates
left, +1 indicates right, and 0 indicates that the level was not present in the
training data
\end{ldescription}
\end{Value}
%
\begin{Section}{Structure}
The following components must be included in a legitimate \code{erboost} object.
\end{Section}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{erboost}{erboost}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{erboost.perf}{erboost performance}{erboost.perf}
\keyword{nonlinear}{erboost.perf}
\keyword{survival}{erboost.perf}
\keyword{nonparametric}{erboost.perf}
\keyword{tree}{erboost.perf}
%
\begin{Description}\relax
Estimates the optimal number of boosting iterations for a \code{erboost} object and
optionally plots various performance measures
\end{Description}
%
\begin{Usage}
\begin{verbatim}
erboost.perf(object, 
         plot.it = TRUE, 
         oobag.curve = FALSE, 
         overlay = TRUE, 
         method)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a \code{\LinkA{erboost.object}{erboost.object}} created from an initial call to 
\code{\LinkA{erboost}{erboost}}.
\item[\code{plot.it}] an indicator of whether or not to plot the performance measures.
Setting \code{plot.it=TRUE} creates two plots. The first plot plots 
\code{object\$train.error} (in black) and \code{object\$valid.error} (in red) 
versus the iteration number. The scale of the error measurement, shown on the 
left vertical axis, depends on the \code{distribution} argument used in the 
initial call to \code{\LinkA{erboost}{erboost}}.
\item[\code{oobag.curve}] indicates whether to plot the out-of-bag performance measures
in a second plot.
\item[\code{overlay}] if TRUE and oobag.curve=TRUE then a right y-axis is added to the 
training and test error plot and the estimated cumulative improvement in the loss 
function is plotted versus the iteration number.
\item[\code{method}] indicate the method used to estimate the optimal number
of boosting iterations. \code{method="OOB"} computes the out-of-bag
estimate and \code{method="test"} uses the test (or validation) dataset 
to compute an out-of-sample estimate. \code{method="cv"} extracts the 
optimal number of iterations using cross-validation if \code{erboost} was called
with \code{cv.folds}>1
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{erboost.perf} returns the estimated optimal number of iterations. The method 
of computation depends on the \code{method} argument.
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{References}\relax
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\bsl{}crG. Ridgeway (2003). "A note on out-of-bag estimation for estimating the optimal
number of boosting iterations," a working paper available at
\url{http://www.i-pensieri.com/gregr/erboost.shtml}.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{erboost}{erboost}}, \code{\LinkA{erboost.object}{erboost.object}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{interact.erboost}{ Estimate the strength of interaction effects }{interact.erboost}
\keyword{methods}{interact.erboost}
%
\begin{Description}\relax
 Computes Friedman's H-statistic to assess the strength of variable interactions. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
interact.erboost(x,
             data,
             i.var = 1,
             n.trees = x$n.trees)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  a \code{\LinkA{erboost.object}{erboost.object}} fitted using a call to \code{\LinkA{erboost}{erboost}}
\item[\code{data}]  the dataset used to construct \code{x}. If the original dataset is
large, a random subsample may be used to accelerate the computation in
\code{interact.erboost}
\item[\code{i.var}] a vector of indices or the names of the variables for compute
the interaction effect. If using indices, the variables are indexed in the
same order that they appear in the initial \code{erboost} formula.
\item[\code{n.trees}]  the number of trees used to generate the plot. Only the first
\code{n.trees} trees will be used
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{interact.erboost} computes Friedman's H-statistic to assess the relative
strength of interaction effects in non-linear models. H is on the scale of
[0-1] with higher values indicating larger interaction effects. To connect to
a more familiar measure, if \eqn{x_1}{} and \eqn{x_2}{} are uncorrelated covariates
with mean 0 and variance 1 and the model is of the form
\deqn{y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3}{}
then
\deqn{H=\frac{\beta_3}{\sqrt{\beta_1^2+\beta_2^2+\beta_3^2}}}{}
\end{Details}
%
\begin{Value}
Returns the value of \eqn{H}{}.
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{References}\relax
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\\{}
J.H. Friedman and B.E. Popescu (2005). ``Predictive Learning via Rule
Ensembles.'' Section 8.1
\end{References}
%
\begin{SeeAlso}\relax
 \code{\LinkA{erboost}{erboost}}, \code{\LinkA{erboost.object}{erboost.object}} 
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{plot.erboost}{ Marginal plots of fitted erboost objects }{plot.erboost}
\keyword{hplot}{plot.erboost}
%
\begin{Description}\relax
Plots the marginal effect of the selected variables by "integrating" out the other variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'erboost'
plot(x,
     i.var = 1,
     n.trees = x$n.trees,
     continuous.resolution = 100,
     return.grid = FALSE,
     ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  a \code{\LinkA{erboost.object}{erboost.object}} fitted using a call to \code{\LinkA{erboost}{erboost}}
\item[\code{i.var}] a vector of indices or the names of the variables to plot. If
using indices, the variables are indexed in the same order that they appear
in the initial \code{erboost} formula.
If \code{length(i.var)} is between 1 and 3 then \code{plot.erboost} produces the plots. Otherwise,
\code{plot.erboost} returns only the grid of evaluation points and their average predictions
\item[\code{n.trees}]  the number of trees used to generate the plot. Only the first
\code{n.trees} trees will be used
\item[\code{continuous.resolution}]  The number of equally space points at which to
evaluate continuous predictors 
\item[\code{return.grid}]  if \code{TRUE} then \code{plot.erboost} produces no graphics and only returns
the grid of evaluation points and their average predictions. This is useful for
customizing the graphics for special variable types or for dimensions greater
than 3 
\item[\code{...}]  other arguments passed to the plot function 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{plot.erboost} produces low dimensional projections of the
\code{\LinkA{erboost.object}{erboost.object}} by integrating out the variables not included in the
\code{i.var} argument. The function selects a grid of points and uses the
weighted tree traversal method described in Friedman (2001) to do the
integration. Based on the variable types included in the projection,
\code{plot.erboost} selects an appropriate display choosing amongst line plots,
contour plots, and \code{\LinkA{lattice}{lattice}} plots. If the default graphics
are not sufficient the user may set \code{return.grid=TRUE}, store the result
of the function, and develop another graphic display more appropriate to the
particular example.
\end{Details}
%
\begin{Value}
Nothing unless \code{return.grid} is true then \code{plot.erboost} produces no
graphics and only returns the grid of evaluation points and their average
predictions.
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{References}\relax
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\\{}
J.H. Friedman (2001). "Greedy Function Approximation: A Gradient Boosting
Machine," Annals of Statistics 29(4).
\end{References}
%
\begin{SeeAlso}\relax
 \code{\LinkA{erboost}{erboost}}, \code{\LinkA{erboost.object}{erboost.object}}, \code{\LinkA{plot}{plot}} 
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{predict.erboost}{ Predict method for erboost Model Fits }{predict.erboost}
\keyword{models}{predict.erboost}
\keyword{regression}{predict.erboost}
%
\begin{Description}\relax
Predicted values based on an ER-Boost Expectile regression model object
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'erboost'
predict(object,
        newdata,
        n.trees,
        single.tree=FALSE,
        ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}]  Object of class inheriting from (\code{\LinkA{erboost.object}{erboost.object}}) 
\item[\code{newdata}]  Data frame of observations for which to make predictions 
\item[\code{n.trees}]  Number of trees used in the prediction. \code{n.trees} may
be a vector in which case predictions are returned for each
iteration specified
\item[\code{single.tree}] If \code{single.tree=TRUE} then \code{predict.erboost} returns
only the predictions from tree(s) \code{n.trees}
\item[\code{...}]  further arguments passed to or from other methods 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{predict.erboost} produces predicted values for each observation in \code{newdata} using the the first \code{n.trees} iterations of the boosting sequence. If \code{n.trees} is a vector than the result is a matrix with each column representing the predictions from erboost models with \code{n.trees[1]} iterations, \code{n.trees[2]} iterations, and so on.

The predictions from \code{erboost} do not include the offset term. The user may add the value of the offset to the predicted value if desired.

If \code{object} was fit using \code{\LinkA{erboost.fit}{erboost.fit}} there will be no
\code{Terms} component. Therefore, the user has greater responsibility to make
sure that \code{newdata} is of the same format (order and number of variables)
as the one originally used to fit the model.
\end{Details}
%
\begin{Value}

Returns a vector of predictions. By default the predictions are on the scale of f(x).
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{erboost}{erboost}}, \code{\LinkA{erboost.object}{erboost.object}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{relative.influence}{ Methods for estimating relative influence }{relative.influence}
\aliasA{erboost.loss}{relative.influence}{erboost.loss}
\aliasA{permutation.test.erboost}{relative.influence}{permutation.test.erboost}
\keyword{hplot}{relative.influence}
%
\begin{Description}\relax
Helper functions for computing the relative influence of each variable in the erboost object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
relative.influence(object, n.trees)
permutation.test.erboost(object, n.trees)
erboost.loss(y,f,w,offset,dist,baseline)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a \code{erboost} object created from an initial call to \code{\LinkA{erboost}{erboost}}.
\item[\code{n.trees}]  the number of trees to use for computations.
\item[\code{y,f,w,offset,dist,baseline}] For \code{erboost.loss}: These components are the
outcome, predicted value, observation weight, offset, distribution, and comparison
loss function, respectively.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is not intended for end-user use. These functions offer the different
methods for computing the relative influence in \code{\LinkA{summary.erboost}{summary.erboost}}.
\code{erboost.loss} is a helper function for \code{permutation.test.erboost}.
\end{Details}
%
\begin{Value}
Returns an unprocessed vector of estimated relative influences.
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{References}\relax
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\\{}
J.H. Friedman (2001). "Greedy Function Approximation: A Gradient Boosting
Machine," Annals of Statistics 29(5):1189-1232.

L. Breiman (2001). "Random Forests," Available at \url{ftp://ftp.stat.berkeley.edu/pub/users/breiman/randomforest2001.pdf}.

\end{References}
%
\begin{SeeAlso}\relax
 \code{\LinkA{summary.erboost}{summary.erboost}} 
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{shrink.erboost}{ L1 shrinkage of the predictor variables in a erboost }{shrink.erboost}
\keyword{methods}{shrink.erboost}
%
\begin{Description}\relax
Performs recursive shrinkage in each of the trees in a erboost fit using different shrinkage parameters for each variable.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
shrink.erboost(object, 
           n.trees, 
           lambda = rep(10, length(object$var.names)), 
           ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}]  A \code{\LinkA{erboost.object}{erboost.object}} 
\item[\code{n.trees}]  the number of trees to use 
\item[\code{lambda}]  a vector with length equal to the number of variables containing the shrinkage parameter for each variable 
\item[\code{...}]  other parameters (ignored) 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This function is currently experimental. Used in conjunction with a gradient ascent search for inclusion of variables.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{predF}] Predicted values from the shrunken tree
\item[\code{objective}] The value of the loss function associated with the predicted values
\item[\code{gradient}] A vector with length equal to the number of variables containing the derivative of the objective function with respect to beta, the logit transform of the shrinkage parameter for each variable
\end{ldescription}
\end{Value}
%
\begin{Section}{Warning}
This function is experimental.
\end{Section}
%
\begin{Author}\relax
 Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu} 
\end{Author}
%
\begin{References}\relax
 
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\\{}
Hastie, T. J., and Pregibon, D. "Shrinking Trees." AT\&T Bell Laboratories Technical Report (March 1990). \url{http://www-stat.stanford.edu/~hastie/Papers/shrinktree.ps} 
\end{References}
%
\begin{SeeAlso}\relax
 \code{\LinkA{shrink.erboost.pred}{shrink.erboost.pred}}, \code{\LinkA{erboost}{erboost}} 
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{shrink.erboost.pred}{ Predictions from a shrunked erboost }{shrink.erboost.pred}
\keyword{methods}{shrink.erboost.pred}
%
\begin{Description}\relax
Makes predictions from a shrunken erboost model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
shrink.erboost.pred(object, 
                newdata, 
                n.trees, 
                lambda = rep(1, length(object$var.names)), 
                ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}]  a \code{\LinkA{erboost.object}{erboost.object}} 
\item[\code{newdata}]  dataset for predictions 
\item[\code{n.trees}]  the number of trees to use 
\item[\code{lambda}]  a vector with length equal to the number of variables containing the shrinkage parameter for each variable 
\item[\code{...}]  other parameters (ignored) 
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A vector with length equal to the number of observations in newdata containing the predictions
\end{Value}
%
\begin{Section}{Warning}
This function is experimental
\end{Section}
%
\begin{Author}\relax
 Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu} 
\end{Author}
%
\begin{SeeAlso}\relax
 \code{\LinkA{shrink.erboost}{shrink.erboost}}, \code{\LinkA{erboost}{erboost}} 
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{summary.erboost}{Summary of a erboost object }{summary.erboost}
\keyword{hplot}{summary.erboost}
%
\begin{Description}\relax
Computes the relative influence of each variable in the erboost object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'erboost'
summary(object,
        cBars=length(object$var.names),
        n.trees=object$n.trees,
        plotit=TRUE,
        order=TRUE,
        method=relative.influence,
        normalize=TRUE,
        ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a \code{erboost} object created from an initial call to
\code{\LinkA{erboost}{erboost}}.
\item[\code{cBars}]  the number of bars to plot. If \code{order=TRUE} the only the
variables with the \code{cBars} largest relative influence will appear in the
barplot. If \code{order=FALSE} then the first \code{cBars} variables will
appear in the plot. In either case, the function will return the relative
influence of all of the variables.
\item[\code{n.trees}]  the number of trees used to generate the plot. Only the first
\code{n.trees} trees will be used.
\item[\code{plotit}]  an indicator as to whether the plot is generated. 
\item[\code{order}]  an indicator as to whether the plotted and/or returned relative
influences are sorted. 
\item[\code{method}]  The function used to compute the relative influence.
\code{\LinkA{relative.influence}{relative.influence}} is the default and is the same as that
described in Friedman (2001). The other current (and experimental) choice is
\code{\LinkA{permutation.test.erboost}{permutation.test.erboost}}. This method randomly permutes each predictor
variable at a time and computes the associated reduction in predictive
performance. This is similar to the variable importance measures Breiman uses
for random forests, but \code{erboost} currently computes using the entire training
dataset (not the out-of-bag observations.
\item[\code{normalize}]  if \code{FALSE} then \code{summary.erboost} returns the 
unnormalized influence. 
\item[\code{...}]  other arguments passed to the plot function. 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This returns the reduction attributeable to each varaible in sum of squared error in 
predicting the gradient on each iteration. It describes the relative influence 
of each variable in reducing the loss function. See the references below for 
exact details on the computation.
\end{Details}
%
\begin{Value}
Returns a data frame where the first component is the variable name and the
second is the computed relative influence, normalized to sum to 100.
\end{Value}
%
\begin{Author}\relax
Yi Yang \email{yiyang@umn.edu} and Hui Zou \email{hzou@stat.umn.edu}
\end{Author}
%
\begin{References}\relax
Yang, Y. and Zou, H. (2013), ``Nonparametric Multiple Expectile Regression via ER-Boost,'' \emph{Journal of Statistical Computation and Simulation}. Accept with minor revisions.

BugReport: \url{http://code.google.com/p/erboost/}\\{}
J.H. Friedman (2001). "Greedy Function Approximation: A Gradient Boosting
Machine," Annals of Statistics 29(5):1189-1232.

L. Breiman (2001). "Random Forests," Available at \url{ftp://ftp.stat.berkeley.edu/pub/users/breiman/randomforest2001.pdf}.

\end{References}
%
\begin{SeeAlso}\relax
 \code{\LinkA{erboost}{erboost}} 
\end{SeeAlso}
